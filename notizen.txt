https://www.youtube.com/watch?v=_8uxMS0anqQ
https://scrapeops.io/python-scrapy-playbook/python-scrapy-indeed-scraper/
scrapy startproject jobportale
scrapy genspider indeed de.indeed.com


?q=Fachinformatiker+Anwendungsentwicklung&l=Frankfurt+am+Main&radius=50&ts=1726823144698&sc=0bf%3Aexrec%28%29%3B&from=searchOnHP&rq=1&rsIdx=0&fromage=last&vjk=abb4e4733a45ae1e

Erklärung:
Die URL verweist auf eine Job-Suchseite von Indeed. Sie zeigt Suchergebnisse für den Jobtitel „Fachinformatiker Anwendungsentwicklung“
in der Region „Frankfurt am Main“ mit einem Suchradius von 50 Kilometern. Die URL enthält folgende Parameter:

    q=Fachinformatiker+Anwendungsentwicklung: Der Suchbegriff ist "Fachinformatiker Anwendungsentwicklung".
    l=Frankfurt+am+Main: Die Suche ist auf Frankfurt am Main beschränkt.
    radius=50: Der Suchradius beträgt 50 Kilometer um Frankfurt.
    ts=1726823144698: Ein Zeitstempel, der die Session oder den Zeitpunkt der Suche markieren könnte.
    sc=0bf%3Aexrec%28%29%3B: Ein Parameter, der wahrscheinlich für die interne Filterung oder Sortierung von Suchergebnissen steht.
    from=searchOnHP: Dieser Parameter zeigt an, dass die Suche von der Hauptseite von Indeed gestartet wurde.
    rq=1 und rsIdx=0: Mögliche Filter- oder Indexwerte, die die Ergebnisse genauer definieren.
    fromage=last: Filtert die Ergebnisse nach kürzlich veröffentlichten Stellenanzeigen.
    vjk=abb4e4733a45ae1e: Dies ist eine eindeutige Job-ID, die auf eine spezifische Stellenanzeige verweist. (Werbung)











https://scrapeops.io/python-scrapy-playbook/python-scrapy-walmart-scraper/#how-to-architect-our-walmart-scraper
pip install scrapeops-scrapy-proxy-sdk

Spider spezifische Einstellungen in der spider.py Datei, anstatt in 'settings.py':
custom_settings = { ... }

    custom_settings = {
        'SCRAPEOPS_API_KEY': YOUR_API_KEY,  # API Key aus einem externen Modul
        'SCRAPEOPS_PROXY_ENABLED': True,  # Aktiviert den ScrapeOps Proxy
        'DOWNLOADER_MIDDLEWARES': {
            'scrapeops_scrapy_proxy_sdk.scrapeops_scrapy_proxy_sdk.ScrapeOpsScrapyProxySdk': 725,  # ScrapeOps Proxy Middleware
        }
    }


quotescraper: http://quotes.toscrape.com/js/
https://www.youtube.com/watch?v=EijzO7n2-dg
    scrapy startproject quotescraper
    cd quotescraper
    scrapy genspider quote quotes.toscrape.com/js/
    scrapy list
    scrapy shell
    fetch('https://quotes.toscrape.com/js/')
    response.body
    

bookscraper: http://books.toscrape.com/
    scrapy startproject bookscraper
    cd bookscraper
    scrapy genspider bookspider books.toscrape.com
    scrapy list
    scrapy shell
    fetch('http://books.toscrape.com/catalogue/category/books_1/')
    response.body  # Check the raw HTML content
    # response.css('article.product_pod h3 a')
    response.css('article.product_pod').get()
    books = response.css('article.product_pod')
    book = books[0]
    book.css('article.product_pod h3 a::attr(title)').get()

    book.css('p.price_color::text').get() # The price of the book
    book.css('article.product_pod h3 a::attr(href)').get() # url of the book
    response.css('li.next a::attr(href)').get()


https://www.youtube.com/watch?v=ExTimuRFn3M&list=PLkhQp3-EGsIi39YF-BE306DDX1xVSTHmn&index=9
https://www.geeksforgeeks.org/scrapy-feed-exports/
https://www.geeksforgeeks.org/scrapy-sending-an-e-mail/


Using pdb (Python Debugger)
The pdb module in Python allows you to run your script in debugging mode from the console.
To use pdb:
 
import pdb; pdb.set_trace()

You can insert the following line in your script wherever you want the debugger to start, usually just before or inside the function you want to debug.
Once in the pdb prompt, you can use the following commands:

    n (next): Execute the next line.
    s (step): Step into the function.
    c (continue): Continue execution until the next breakpoint.
    p <variable>: Print the value of a variable.

Using Logging
For more advanced debugging and to keep track of different levels of information (debug, info, warning, error), you can use the logging module.

scrapy crawl chocolatespider
scrapy crawl chocolatespider -O test.csv


# feedexport.py:
params["mtime"] = params["time"].split('+')[0] 
# Neuer Parameter mtime, Zeitzone abschneiden (+00-00)

# chocolatespider.py:
custom_settings = {
    'FEEDS':    {'../data/%(name)s/%(name)s_%(mtime)s.csv': {'format': 'csv',}}
    }
